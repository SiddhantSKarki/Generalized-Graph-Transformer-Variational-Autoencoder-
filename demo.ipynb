{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a307c61-a382-45d6-9520-5697ed1ac185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.utils as utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import seaborn as sns\n",
    "import args\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba35429-b904-4e55-835e-74e5ff28475b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "def prepare_gtnvae_data(root, dataset_name='Cora', pos_dim=16):\n",
    "    from torch_geometric.datasets import Planetoid\n",
    "    from torch_geometric.utils import to_dense_adj, train_test_split_edges, get_laplacian\n",
    "\n",
    "    dataset = Planetoid(root=root, name=dataset_name)\n",
    "    data = dataset[0]\n",
    "    data_split = train_test_split_edges(data)\n",
    "    N = data.num_nodes\n",
    "\n",
    "    # Dense node features\n",
    "    x = data.x.unsqueeze(0)\n",
    "\n",
    "    # Dense adjacencies\n",
    "    def to_dense_binary(edge_index):\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=N)[0]\n",
    "        return (adj > 0).float()\n",
    "\n",
    "    train_adj = to_dense_binary(data_split.train_pos_edge_index).unsqueeze(0)\n",
    "    val_adj   = to_dense_binary(data_split.val_pos_edge_index).unsqueeze(0)\n",
    "    test_adj  = to_dense_binary(data_split.test_pos_edge_index).unsqueeze(0)\n",
    "\n",
    "    # Positional encodings from train edges\n",
    "    edge_index, edge_weight = get_laplacian(data_split.train_pos_edge_index, normalization='sym', num_nodes=N)\n",
    "    L = torch.sparse_coo_tensor(edge_index, edge_weight, (N, N)).to_dense()\n",
    "    eigval, eigvec = torch.linalg.eigh(L)\n",
    "    pos_enc = eigvec[:, 1:pos_dim+1].unsqueeze(0)\n",
    "\n",
    "    return x, pos_enc, train_adj, val_adj, test_adj, data.y\n",
    "\n",
    "x, pos_enc, train_adj, val_adj, test_adj, y = prepare_gtnvae_data(\"../data/\", 'Cora', pos_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8634a77-f40a-4a56-b7c1-80c8b05bc9b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DATASET SUMMARY =====\n",
      "Node Feature Shape (x):           torch.Size([1, 2708, 1433])  -> [B, N, d_node_in]\n",
      "Positional Encoding Shape:        torch.Size([1, 2708, 128])  -> [B, N, d_pos]\n",
      "Train Adjacency Shape:            torch.Size([1, 2708, 2708])  -> [B, N, N]\n",
      "Validation Adjacency Shape:       torch.Size([1, 2708, 2708])  -> [B, N, N]\n",
      "Test Adjacency Shape:             torch.Size([1, 2708, 2708])  -> [B, N, N]\n",
      "--------------------------------------\n",
      "Total Nodes (N):                  2708\n",
      "Feature Dimension (d_node_in):    1433\n",
      "Positional Dim (d_pos):           128\n",
      "Total Train Edges:                8976\n",
      "Total Val Edges:                  263\n",
      "Total Test Edges:                 527\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"\\n===== DATASET SUMMARY =====\\n\"\n",
    "    f\"Node Feature Shape (x):           {x.shape}  -> [B, N, d_node_in]\\n\"\n",
    "    f\"Positional Encoding Shape:        {pos_enc.shape}  -> [B, N, d_pos]\\n\"\n",
    "    f\"Train Adjacency Shape:            {train_adj.shape}  -> [B, N, N]\\n\"\n",
    "    f\"Validation Adjacency Shape:       {val_adj.shape}  -> [B, N, N]\\n\"\n",
    "    f\"Test Adjacency Shape:             {test_adj.shape}  -> [B, N, N]\\n\"\n",
    "    f\"--------------------------------------\\n\"\n",
    "    f\"Total Nodes (N):                  {train_adj.size(-1)}\\n\"\n",
    "    f\"Feature Dimension (d_node_in):    {x.size(-1)}\\n\"\n",
    "    f\"Positional Dim (d_pos):           {pos_enc.size(-1)}\\n\"\n",
    "    f\"Total Train Edges:                {int(train_adj.sum().item())}\\n\"\n",
    "    f\"Total Val Edges:                  {int(val_adj.sum().item())}\\n\"\n",
    "    f\"Total Test Edges:                 {int(test_adj.sum().item())}\\n\"\n",
    "    f\"======================================\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eec6cd3-46c5-4f42-b145-43fb30a34b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"mps\")  # or \"cuda\" / \"cpu\"\n",
    "load = None\n",
    "\n",
    "model = GTN_VAE(\n",
    "    input_dim=x.size(-1),\n",
    "    pos_dim=pos_enc.size(-1),\n",
    "    hidden_dim=128,\n",
    "    n_layers=4,\n",
    "    n_heads=4\n",
    ").to(device)\n",
    "\n",
    "if load:\n",
    "    model = torch.load(load, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d8489f-aabe-43f8-b6b8-7791d48fb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, pos_enc, train_adj, val_adj, test_adj, y = (\n",
    "    x.to(device),\n",
    "    pos_enc.to(device),\n",
    "    train_adj.to(device),\n",
    "    val_adj.to(device),\n",
    "    test_adj.to(device),\n",
    "    y.to(device)\n",
    ")\n",
    "\n",
    "# Weighting setup\n",
    "pos_weight = float(train_adj.numel() - train_adj.sum()) / train_adj.sum()\n",
    "norm = train_adj.numel() / ((train_adj.numel() - train_adj.sum()) * 2)\n",
    "weight_mask = train_adj.flatten() == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0), device=device)\n",
    "weight_tensor[weight_mask] = pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0622b8c9-2021-48ec-9a67-08baed3894cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b7e9bef-ffe5-4076-954b-59eb9d1d34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def compute_metrics(adj_pred, adj_true):\n",
    "    adj_true_np = adj_true.detach().cpu().numpy().flatten()\n",
    "    adj_pred_np = adj_pred.detach().cpu().numpy().flatten()\n",
    "    roc = roc_auc_score(adj_true_np, adj_pred_np)\n",
    "    ap = average_precision_score(adj_true_np, adj_pred_np)\n",
    "    return roc, ap\n",
    "\n",
    "def compute_accuracy(adj_pred, adj_true, threshold=0.5):\n",
    "    pred_bin = (adj_pred > threshold).float()\n",
    "    correct = (pred_bin == adj_true).float().sum()\n",
    "    return (correct / adj_true.numel()).item()\n",
    "\n",
    "def weighted_vae_loss(adj_recon, adj_true, mu, logvar, weight_tensor, norm):\n",
    "    bce = F.binary_cross_entropy(\n",
    "        adj_recon.view(-1),\n",
    "        adj_true.view(-1),\n",
    "        weight=weight_tensor,\n",
    "    )\n",
    "    recon_loss = norm * bce\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "\n",
    "def balanced_edge_indices(adj_true, num_samples=5000):\n",
    "    \"\"\"\n",
    "    Returns equal numbers of positive and negative edge indices.\n",
    "    Ensures class balance and guards against small graphs.\n",
    "    \"\"\"\n",
    "    # Flatten and find all positive/negative indices\n",
    "    pos_idx = (adj_true.view(-1) == 1).nonzero(as_tuple=False).view(-1)\n",
    "    neg_idx = (adj_true.view(-1) == 0).nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "    # How many to sample per class\n",
    "    num_pos = min(len(pos_idx), num_samples // 2)\n",
    "    num_neg = num_pos  # strict equality\n",
    "    if num_pos == 0 or num_neg == 0:\n",
    "        raise ValueError(\"No positive or negative edges to sample from.\")\n",
    "\n",
    "    # Random balanced sample\n",
    "    pos_idx = pos_idx[torch.randperm(len(pos_idx))[:num_pos]]\n",
    "    neg_idx = neg_idx[torch.randperm(len(neg_idx))[:num_neg]]\n",
    "    idx = torch.cat([pos_idx, neg_idx])\n",
    "\n",
    "    return idx, num_pos, num_neg\n",
    "\n",
    "\n",
    "def sampled_vae_loss(adj_pred, adj_true, mu, logvar, num_samples=5000):\n",
    "    \"\"\"\n",
    "    Balanced random BCE + KL loss for VGAE-style models.\n",
    "    \"\"\"\n",
    "    idx, num_pos, num_neg = balanced_edge_indices(adj_true, num_samples)\n",
    "    y_true = adj_true.view(-1)[idx]\n",
    "    y_pred = adj_pred.view(-1)[idx]\n",
    "\n",
    "    # BCE over balanced sample\n",
    "    bce = F.binary_cross_entropy(y_pred, y_true)\n",
    "    kl  = -0.001 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return bce + kl, bce, kl\n",
    "\n",
    "\n",
    "def sampled_metrics(adj_pred, adj_true, num_samples=5000):\n",
    "    \"\"\"\n",
    "    Computes ROC-AUC, AP, and Accuracy on balanced edge samples.\n",
    "    \"\"\"\n",
    "    idx, num_pos, num_neg = balanced_edge_indices(adj_true, num_samples)\n",
    "    y_true = adj_true.view(-1)[idx].cpu().numpy()\n",
    "    y_pred = adj_pred.view(-1)[idx].detach().cpu().numpy()\n",
    "\n",
    "    roc = roc_auc_score(y_true, y_pred)\n",
    "    ap  = average_precision_score(y_true, y_pred)\n",
    "    acc = ((torch.tensor(y_pred) > 0.5).float().numpy() == y_true).mean()\n",
    "    return roc, ap, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130b67c7-ee07-4110-8db7-0b1da23f1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_in_out(model_comp, inputs):\n",
    "    output = model_comp(*inputs)\n",
    "    output_mean = output.mean()\n",
    "    output_std = output.std()\n",
    "    print(f\"output: {output}\\noutput_mean:{output_mean}\\noutput_std:{output_std}\")\n",
    "    return output\n",
    "\n",
    "def model_debug_auto(model_comp, inputs, verbose=True):\n",
    "    # Run forward pass\n",
    "    if not isinstance(inputs, (tuple, list)):\n",
    "        inputs = (inputs,)\n",
    "    \n",
    "    output = model_comp(*inputs)\n",
    "    \n",
    "    def summarize_tensor(name, tensor):\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            return {\n",
    "                \"name\": name,\n",
    "                \"shape\": tuple(tensor.shape),\n",
    "                \"dtype\": tensor.dtype,\n",
    "                \"min\": tensor.min().item(),\n",
    "                \"max\": tensor.max().item(),\n",
    "                \"mean\": tensor.mean().item(),\n",
    "                \"std\": tensor.std().item()\n",
    "            }\n",
    "        elif isinstance(tensor, (list, tuple)):\n",
    "            return [summarize_tensor(f\"{name}[{i}]\", t) for i, t in enumerate(tensor)]\n",
    "        elif isinstance(tensor, dict):\n",
    "            return {k: summarize_tensor(f\"{name}.{k}\", v) for k, v in tensor.items()}\n",
    "        else:\n",
    "            return {name: str(type(tensor))}\n",
    "\n",
    "    # Summarize input(s) and output(s)\n",
    "    input_summary = [summarize_tensor(f\"input[{i}]\", inp) for i, inp in enumerate(inputs)]\n",
    "    output_summary = summarize_tensor(\"output\", output)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== INPUT SUMMARY ===\")\n",
    "        for s in input_summary:\n",
    "            print(s)\n",
    "        print(\"\\n=== OUTPUT SUMMARY ===\")\n",
    "        print(output_summary)\n",
    "    \n",
    "    return output, input_summary, output_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902d4e4-e5fd-4d91-bc3b-1147d3578ab1",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c2a1d19-6a9a-4855-849b-8e4ba92ee8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [001/200] | Train Loss: 4.7473 | Val Loss: 4.5702 | Train Acc: 0.5032 | Val Acc: 0.4950 | Train ROC: 0.5038 | Val ROC: 0.5086 | Train AP: 0.5019 | Val AP: 0.5080\n",
      "Epoch [002/200] | Train Loss: 4.2339 | Val Loss: 3.9358 | Train Acc: 0.5180 | Val Acc: 0.4975 | Train ROC: 0.5258 | Val ROC: 0.5035 | Train AP: 0.5148 | Val AP: 0.5013\n",
      "Epoch [003/200] | Train Loss: 3.8169 | Val Loss: 3.7201 | Train Acc: 0.5201 | Val Acc: 0.5600 | Train ROC: 0.5242 | Val ROC: 0.5543 | Train AP: 0.5154 | Val AP: 0.5398\n",
      "Epoch [004/200] | Train Loss: 3.8571 | Val Loss: 3.8252 | Train Acc: 0.5051 | Val Acc: 0.5075 | Train ROC: 0.5136 | Val ROC: 0.4986 | Train AP: 0.5097 | Val AP: 0.4936\n",
      "Epoch [005/200] | Train Loss: 3.8008 | Val Loss: 3.2760 | Train Acc: 0.5224 | Val Acc: 0.5150 | Train ROC: 0.5229 | Val ROC: 0.5214 | Train AP: 0.5139 | Val AP: 0.5096\n",
      "Epoch [006/200] | Train Loss: 3.6599 | Val Loss: 3.3327 | Train Acc: 0.5200 | Val Acc: 0.5275 | Train ROC: 0.5248 | Val ROC: 0.5325 | Train AP: 0.5153 | Val AP: 0.5170\n",
      "Epoch [007/200] | Train Loss: 3.5054 | Val Loss: 3.3646 | Train Acc: 0.5250 | Val Acc: 0.5650 | Train ROC: 0.5369 | Val ROC: 0.5691 | Train AP: 0.5258 | Val AP: 0.5489\n",
      "Epoch [008/200] | Train Loss: 3.4249 | Val Loss: 2.6679 | Train Acc: 0.5238 | Val Acc: 0.5850 | Train ROC: 0.5366 | Val ROC: 0.6231 | Train AP: 0.5276 | Val AP: 0.5989\n",
      "Epoch [009/200] | Train Loss: 3.2076 | Val Loss: 3.0307 | Train Acc: 0.5231 | Val Acc: 0.5050 | Train ROC: 0.5333 | Val ROC: 0.5179 | Train AP: 0.5237 | Val AP: 0.5134\n",
      "Epoch [010/200] | Train Loss: 3.1123 | Val Loss: 2.9579 | Train Acc: 0.5375 | Val Acc: 0.5100 | Train ROC: 0.5517 | Val ROC: 0.5313 | Train AP: 0.5417 | Val AP: 0.5330\n",
      "Epoch [011/200] | Train Loss: 3.0106 | Val Loss: 2.9048 | Train Acc: 0.5479 | Val Acc: 0.5400 | Train ROC: 0.5712 | Val ROC: 0.5689 | Train AP: 0.5547 | Val AP: 0.5622\n",
      "Epoch [012/200] | Train Loss: 2.7549 | Val Loss: 2.4975 | Train Acc: 0.5654 | Val Acc: 0.6100 | Train ROC: 0.5989 | Val ROC: 0.6288 | Train AP: 0.5780 | Val AP: 0.5860\n",
      "Epoch [013/200] | Train Loss: 2.6854 | Val Loss: 2.7089 | Train Acc: 0.5656 | Val Acc: 0.5575 | Train ROC: 0.5984 | Val ROC: 0.5936 | Train AP: 0.5782 | Val AP: 0.5871\n",
      "Epoch [014/200] | Train Loss: 2.4038 | Val Loss: 2.3044 | Train Acc: 0.5816 | Val Acc: 0.6200 | Train ROC: 0.6206 | Val ROC: 0.6520 | Train AP: 0.6012 | Val AP: 0.6164\n",
      "Epoch [015/200] | Train Loss: 2.2148 | Val Loss: 2.3091 | Train Acc: 0.6074 | Val Acc: 0.6000 | Train ROC: 0.6619 | Val ROC: 0.6424 | Train AP: 0.6342 | Val AP: 0.6232\n",
      "Epoch [016/200] | Train Loss: 2.1184 | Val Loss: 1.8625 | Train Acc: 0.6118 | Val Acc: 0.6025 | Train ROC: 0.6691 | Val ROC: 0.6370 | Train AP: 0.6467 | Val AP: 0.6092\n",
      "Epoch [017/200] | Train Loss: 2.0041 | Val Loss: 1.7544 | Train Acc: 0.6049 | Val Acc: 0.5650 | Train ROC: 0.6679 | Val ROC: 0.6627 | Train AP: 0.6449 | Val AP: 0.6864\n",
      "Epoch [018/200] | Train Loss: 1.9074 | Val Loss: 1.6562 | Train Acc: 0.6226 | Val Acc: 0.6050 | Train ROC: 0.7014 | Val ROC: 0.6578 | Train AP: 0.6857 | Val AP: 0.6358\n",
      "Epoch [019/200] | Train Loss: 1.7303 | Val Loss: 1.6524 | Train Acc: 0.6325 | Val Acc: 0.6375 | Train ROC: 0.7028 | Val ROC: 0.6988 | Train AP: 0.6825 | Val AP: 0.6994\n",
      "Epoch [020/200] | Train Loss: 1.5853 | Val Loss: 1.4054 | Train Acc: 0.6204 | Val Acc: 0.6125 | Train ROC: 0.6891 | Val ROC: 0.6668 | Train AP: 0.6753 | Val AP: 0.6624\n",
      "Epoch [021/200] | Train Loss: 1.4474 | Val Loss: 1.3712 | Train Acc: 0.6195 | Val Acc: 0.5975 | Train ROC: 0.6851 | Val ROC: 0.6646 | Train AP: 0.6727 | Val AP: 0.6740\n",
      "Epoch [022/200] | Train Loss: 1.2848 | Val Loss: 1.2050 | Train Acc: 0.6105 | Val Acc: 0.6000 | Train ROC: 0.6790 | Val ROC: 0.6511 | Train AP: 0.6700 | Val AP: 0.6388\n",
      "Epoch [023/200] | Train Loss: 1.1980 | Val Loss: 1.2242 | Train Acc: 0.6111 | Val Acc: 0.6125 | Train ROC: 0.6732 | Val ROC: 0.6881 | Train AP: 0.6665 | Val AP: 0.6832\n",
      "Epoch [024/200] | Train Loss: 1.1443 | Val Loss: 1.0233 | Train Acc: 0.5910 | Val Acc: 0.6100 | Train ROC: 0.6501 | Val ROC: 0.6746 | Train AP: 0.6512 | Val AP: 0.6734\n",
      "Epoch [025/200] | Train Loss: 1.0549 | Val Loss: 1.0347 | Train Acc: 0.6081 | Val Acc: 0.5875 | Train ROC: 0.6729 | Val ROC: 0.6379 | Train AP: 0.6729 | Val AP: 0.6319\n",
      "Epoch [026/200] | Train Loss: 0.9516 | Val Loss: 0.8881 | Train Acc: 0.5954 | Val Acc: 0.5775 | Train ROC: 0.6602 | Val ROC: 0.6202 | Train AP: 0.6566 | Val AP: 0.6215\n",
      "Epoch [027/200] | Train Loss: 0.9034 | Val Loss: 0.8101 | Train Acc: 0.6008 | Val Acc: 0.5975 | Train ROC: 0.6596 | Val ROC: 0.6526 | Train AP: 0.6598 | Val AP: 0.6616\n",
      "Epoch [028/200] | Train Loss: 0.8543 | Val Loss: 0.9014 | Train Acc: 0.6075 | Val Acc: 0.5725 | Train ROC: 0.6686 | Val ROC: 0.6353 | Train AP: 0.6592 | Val AP: 0.6588\n",
      "Epoch [029/200] | Train Loss: 0.8098 | Val Loss: 0.8435 | Train Acc: 0.6124 | Val Acc: 0.5950 | Train ROC: 0.6836 | Val ROC: 0.6321 | Train AP: 0.6834 | Val AP: 0.6105\n",
      "Epoch [030/200] | Train Loss: 0.8008 | Val Loss: 0.8708 | Train Acc: 0.6118 | Val Acc: 0.5750 | Train ROC: 0.6761 | Val ROC: 0.6407 | Train AP: 0.6686 | Val AP: 0.6618\n",
      "Epoch [031/200] | Train Loss: 0.7640 | Val Loss: 0.7280 | Train Acc: 0.6175 | Val Acc: 0.5975 | Train ROC: 0.6879 | Val ROC: 0.6796 | Train AP: 0.6845 | Val AP: 0.6611\n",
      "Epoch [032/200] | Train Loss: 0.7209 | Val Loss: 0.7275 | Train Acc: 0.6336 | Val Acc: 0.6075 | Train ROC: 0.7095 | Val ROC: 0.6746 | Train AP: 0.7123 | Val AP: 0.6551\n",
      "Epoch [033/200] | Train Loss: 0.6953 | Val Loss: 0.6951 | Train Acc: 0.6450 | Val Acc: 0.6675 | Train ROC: 0.7310 | Val ROC: 0.7291 | Train AP: 0.7239 | Val AP: 0.6996\n",
      "Epoch [034/200] | Train Loss: 0.6844 | Val Loss: 0.6632 | Train Acc: 0.6518 | Val Acc: 0.6300 | Train ROC: 0.7472 | Val ROC: 0.7050 | Train AP: 0.7440 | Val AP: 0.7105\n",
      "Epoch [035/200] | Train Loss: 0.6606 | Val Loss: 0.6757 | Train Acc: 0.6597 | Val Acc: 0.6650 | Train ROC: 0.7642 | Val ROC: 0.7637 | Train AP: 0.7603 | Val AP: 0.7603\n",
      "Epoch [036/200] | Train Loss: 0.6495 | Val Loss: 0.6442 | Train Acc: 0.6756 | Val Acc: 0.7125 | Train ROC: 0.7865 | Val ROC: 0.7824 | Train AP: 0.7843 | Val AP: 0.7576\n",
      "Epoch [037/200] | Train Loss: 0.6148 | Val Loss: 0.6241 | Train Acc: 0.6850 | Val Acc: 0.6950 | Train ROC: 0.8064 | Val ROC: 0.7846 | Train AP: 0.8056 | Val AP: 0.7729\n",
      "Epoch [038/200] | Train Loss: 0.6147 | Val Loss: 0.6012 | Train Acc: 0.6847 | Val Acc: 0.6775 | Train ROC: 0.8095 | Val ROC: 0.7910 | Train AP: 0.8056 | Val AP: 0.7733\n",
      "Epoch [039/200] | Train Loss: 0.6076 | Val Loss: 0.6166 | Train Acc: 0.7061 | Val Acc: 0.6625 | Train ROC: 0.8337 | Val ROC: 0.7990 | Train AP: 0.8248 | Val AP: 0.8028\n",
      "Epoch [040/200] | Train Loss: 0.5807 | Val Loss: 0.6995 | Train Acc: 0.7091 | Val Acc: 0.6550 | Train ROC: 0.8425 | Val ROC: 0.7713 | Train AP: 0.8366 | Val AP: 0.7657\n",
      "Epoch [041/200] | Train Loss: 0.5896 | Val Loss: 0.6300 | Train Acc: 0.7060 | Val Acc: 0.6675 | Train ROC: 0.8468 | Val ROC: 0.7812 | Train AP: 0.8401 | Val AP: 0.7864\n",
      "Epoch [042/200] | Train Loss: 0.5812 | Val Loss: 0.6056 | Train Acc: 0.7074 | Val Acc: 0.7100 | Train ROC: 0.8399 | Val ROC: 0.8135 | Train AP: 0.8351 | Val AP: 0.8111\n",
      "Epoch [043/200] | Train Loss: 0.5784 | Val Loss: 0.5745 | Train Acc: 0.7120 | Val Acc: 0.7050 | Train ROC: 0.8555 | Val ROC: 0.8221 | Train AP: 0.8534 | Val AP: 0.8221\n",
      "Epoch [044/200] | Train Loss: 0.5758 | Val Loss: 0.6569 | Train Acc: 0.7113 | Val Acc: 0.6625 | Train ROC: 0.8590 | Val ROC: 0.7999 | Train AP: 0.8593 | Val AP: 0.7999\n",
      "Epoch [045/200] | Train Loss: 0.5531 | Val Loss: 0.6239 | Train Acc: 0.7145 | Val Acc: 0.6925 | Train ROC: 0.8687 | Val ROC: 0.8189 | Train AP: 0.8646 | Val AP: 0.8031\n",
      "Epoch [046/200] | Train Loss: 0.5534 | Val Loss: 0.5861 | Train Acc: 0.7110 | Val Acc: 0.7200 | Train ROC: 0.8636 | Val ROC: 0.8331 | Train AP: 0.8623 | Val AP: 0.8315\n",
      "Epoch [047/200] | Train Loss: 0.5486 | Val Loss: 0.5734 | Train Acc: 0.7179 | Val Acc: 0.7275 | Train ROC: 0.8677 | Val ROC: 0.8220 | Train AP: 0.8679 | Val AP: 0.7973\n",
      "Epoch [048/200] | Train Loss: 0.5472 | Val Loss: 0.5413 | Train Acc: 0.7151 | Val Acc: 0.7325 | Train ROC: 0.8689 | Val ROC: 0.8527 | Train AP: 0.8633 | Val AP: 0.8380\n",
      "Epoch [049/200] | Train Loss: 0.5407 | Val Loss: 0.5544 | Train Acc: 0.7124 | Val Acc: 0.6750 | Train ROC: 0.8621 | Val ROC: 0.8190 | Train AP: 0.8610 | Val AP: 0.8074\n",
      "Epoch [050/200] | Train Loss: 0.5461 | Val Loss: 0.5574 | Train Acc: 0.7221 | Val Acc: 0.7075 | Train ROC: 0.8764 | Val ROC: 0.8340 | Train AP: 0.8725 | Val AP: 0.8370\n",
      "Epoch [051/200] | Train Loss: 0.5337 | Val Loss: 0.5253 | Train Acc: 0.7216 | Val Acc: 0.7150 | Train ROC: 0.8768 | Val ROC: 0.8259 | Train AP: 0.8724 | Val AP: 0.8077\n",
      "Epoch [052/200] | Train Loss: 0.5297 | Val Loss: 0.5498 | Train Acc: 0.7320 | Val Acc: 0.7250 | Train ROC: 0.8877 | Val ROC: 0.8464 | Train AP: 0.8831 | Val AP: 0.8418\n",
      "Epoch [053/200] | Train Loss: 0.5272 | Val Loss: 0.5329 | Train Acc: 0.7299 | Val Acc: 0.7175 | Train ROC: 0.8873 | Val ROC: 0.8497 | Train AP: 0.8814 | Val AP: 0.8239\n",
      "Epoch [054/200] | Train Loss: 0.5180 | Val Loss: 0.5579 | Train Acc: 0.7301 | Val Acc: 0.7225 | Train ROC: 0.8907 | Val ROC: 0.8709 | Train AP: 0.8825 | Val AP: 0.8629\n",
      "Epoch [055/200] | Train Loss: 0.5217 | Val Loss: 0.5393 | Train Acc: 0.7265 | Val Acc: 0.6950 | Train ROC: 0.8924 | Val ROC: 0.8467 | Train AP: 0.8885 | Val AP: 0.8467\n",
      "Epoch [056/200] | Train Loss: 0.5204 | Val Loss: 0.5435 | Train Acc: 0.7291 | Val Acc: 0.7325 | Train ROC: 0.8969 | Val ROC: 0.8489 | Train AP: 0.8922 | Val AP: 0.8408\n",
      "Epoch [057/200] | Train Loss: 0.5161 | Val Loss: 0.5388 | Train Acc: 0.7310 | Val Acc: 0.7275 | Train ROC: 0.8966 | Val ROC: 0.8613 | Train AP: 0.8922 | Val AP: 0.8689\n",
      "Epoch [058/200] | Train Loss: 0.5245 | Val Loss: 0.5691 | Train Acc: 0.7405 | Val Acc: 0.6800 | Train ROC: 0.9046 | Val ROC: 0.8418 | Train AP: 0.9013 | Val AP: 0.8468\n",
      "Epoch [059/200] | Train Loss: 0.5031 | Val Loss: 0.5521 | Train Acc: 0.7399 | Val Acc: 0.7100 | Train ROC: 0.9106 | Val ROC: 0.8448 | Train AP: 0.9070 | Val AP: 0.8495\n",
      "Epoch [060/200] | Train Loss: 0.5127 | Val Loss: 0.5270 | Train Acc: 0.7371 | Val Acc: 0.6950 | Train ROC: 0.9120 | Val ROC: 0.8503 | Train AP: 0.9095 | Val AP: 0.8482\n",
      "Epoch [061/200] | Train Loss: 0.5131 | Val Loss: 0.5518 | Train Acc: 0.7350 | Val Acc: 0.7100 | Train ROC: 0.9091 | Val ROC: 0.8441 | Train AP: 0.9055 | Val AP: 0.8410\n",
      "Epoch [062/200] | Train Loss: 0.5064 | Val Loss: 0.5525 | Train Acc: 0.7494 | Val Acc: 0.7000 | Train ROC: 0.9200 | Val ROC: 0.8449 | Train AP: 0.9179 | Val AP: 0.8577\n",
      "Epoch [063/200] | Train Loss: 0.5000 | Val Loss: 0.5346 | Train Acc: 0.7440 | Val Acc: 0.7350 | Train ROC: 0.9131 | Val ROC: 0.8687 | Train AP: 0.9089 | Val AP: 0.8685\n",
      "Epoch [064/200] | Train Loss: 0.5146 | Val Loss: 0.5355 | Train Acc: 0.7395 | Val Acc: 0.7550 | Train ROC: 0.9160 | Val ROC: 0.9029 | Train AP: 0.9125 | Val AP: 0.9001\n",
      "Epoch [065/200] | Train Loss: 0.5091 | Val Loss: 0.5034 | Train Acc: 0.7450 | Val Acc: 0.7225 | Train ROC: 0.9223 | Val ROC: 0.8605 | Train AP: 0.9188 | Val AP: 0.8702\n",
      "Epoch [066/200] | Train Loss: 0.4992 | Val Loss: 0.5410 | Train Acc: 0.7398 | Val Acc: 0.7100 | Train ROC: 0.9136 | Val ROC: 0.8792 | Train AP: 0.9102 | Val AP: 0.8805\n",
      "Epoch [067/200] | Train Loss: 0.5031 | Val Loss: 0.5904 | Train Acc: 0.7478 | Val Acc: 0.7125 | Train ROC: 0.9250 | Val ROC: 0.8676 | Train AP: 0.9223 | Val AP: 0.8681\n",
      "Epoch [068/200] | Train Loss: 0.4933 | Val Loss: 0.5149 | Train Acc: 0.7401 | Val Acc: 0.7325 | Train ROC: 0.9243 | Val ROC: 0.8607 | Train AP: 0.9195 | Val AP: 0.8616\n",
      "Epoch [069/200] | Train Loss: 0.4952 | Val Loss: 0.5214 | Train Acc: 0.7401 | Val Acc: 0.7375 | Train ROC: 0.9265 | Val ROC: 0.8696 | Train AP: 0.9240 | Val AP: 0.8674\n",
      "Epoch [070/200] | Train Loss: 0.5024 | Val Loss: 0.5386 | Train Acc: 0.7508 | Val Acc: 0.7500 | Train ROC: 0.9169 | Val ROC: 0.9055 | Train AP: 0.9083 | Val AP: 0.9080\n",
      "Epoch [071/200] | Train Loss: 0.4900 | Val Loss: 0.5526 | Train Acc: 0.7474 | Val Acc: 0.7275 | Train ROC: 0.9240 | Val ROC: 0.8739 | Train AP: 0.9189 | Val AP: 0.8745\n",
      "Epoch [072/200] | Train Loss: 0.4924 | Val Loss: 0.5469 | Train Acc: 0.7489 | Val Acc: 0.7100 | Train ROC: 0.9341 | Val ROC: 0.8808 | Train AP: 0.9324 | Val AP: 0.8836\n",
      "Epoch [073/200] | Train Loss: 0.4909 | Val Loss: 0.5135 | Train Acc: 0.7534 | Val Acc: 0.7350 | Train ROC: 0.9314 | Val ROC: 0.8684 | Train AP: 0.9271 | Val AP: 0.8729\n",
      "Epoch [074/200] | Train Loss: 0.4961 | Val Loss: 0.5592 | Train Acc: 0.7465 | Val Acc: 0.7400 | Train ROC: 0.9320 | Val ROC: 0.8722 | Train AP: 0.9292 | Val AP: 0.8671\n",
      "Epoch [075/200] | Train Loss: 0.4844 | Val Loss: 0.5023 | Train Acc: 0.7462 | Val Acc: 0.7175 | Train ROC: 0.9302 | Val ROC: 0.8661 | Train AP: 0.9260 | Val AP: 0.8662\n",
      "Epoch [076/200] | Train Loss: 0.4803 | Val Loss: 0.5281 | Train Acc: 0.7516 | Val Acc: 0.7550 | Train ROC: 0.9371 | Val ROC: 0.9093 | Train AP: 0.9327 | Val AP: 0.9155\n",
      "Epoch [077/200] | Train Loss: 0.4832 | Val Loss: 0.5060 | Train Acc: 0.7496 | Val Acc: 0.7650 | Train ROC: 0.9394 | Val ROC: 0.9161 | Train AP: 0.9342 | Val AP: 0.9175\n",
      "Epoch [078/200] | Train Loss: 0.4799 | Val Loss: 0.5315 | Train Acc: 0.7538 | Val Acc: 0.7100 | Train ROC: 0.9358 | Val ROC: 0.8675 | Train AP: 0.9314 | Val AP: 0.8688\n",
      "Epoch [079/200] | Train Loss: 0.4862 | Val Loss: 0.4889 | Train Acc: 0.7549 | Val Acc: 0.7025 | Train ROC: 0.9428 | Val ROC: 0.8788 | Train AP: 0.9388 | Val AP: 0.8585\n",
      "Epoch [080/200] | Train Loss: 0.4965 | Val Loss: 0.5384 | Train Acc: 0.7530 | Val Acc: 0.7000 | Train ROC: 0.9388 | Val ROC: 0.8758 | Train AP: 0.9346 | Val AP: 0.8832\n",
      "Epoch [081/200] | Train Loss: 0.4882 | Val Loss: 0.5029 | Train Acc: 0.7565 | Val Acc: 0.7375 | Train ROC: 0.9421 | Val ROC: 0.8950 | Train AP: 0.9386 | Val AP: 0.8938\n",
      "Epoch [082/200] | Train Loss: 0.4805 | Val Loss: 0.4852 | Train Acc: 0.7469 | Val Acc: 0.7550 | Train ROC: 0.9449 | Val ROC: 0.8960 | Train AP: 0.9397 | Val AP: 0.8904\n",
      "Epoch [083/200] | Train Loss: 0.4867 | Val Loss: 0.5160 | Train Acc: 0.7561 | Val Acc: 0.7175 | Train ROC: 0.9492 | Val ROC: 0.8793 | Train AP: 0.9455 | Val AP: 0.8874\n",
      "Epoch [084/200] | Train Loss: 0.4854 | Val Loss: 0.5236 | Train Acc: 0.7475 | Val Acc: 0.7625 | Train ROC: 0.9434 | Val ROC: 0.8874 | Train AP: 0.9398 | Val AP: 0.8712\n",
      "Epoch [085/200] | Train Loss: 0.4831 | Val Loss: 0.5087 | Train Acc: 0.7465 | Val Acc: 0.7225 | Train ROC: 0.9447 | Val ROC: 0.8774 | Train AP: 0.9418 | Val AP: 0.8740\n",
      "Epoch [086/200] | Train Loss: 0.4809 | Val Loss: 0.5170 | Train Acc: 0.7456 | Val Acc: 0.7550 | Train ROC: 0.9493 | Val ROC: 0.8986 | Train AP: 0.9464 | Val AP: 0.8810\n",
      "Epoch [087/200] | Train Loss: 0.4772 | Val Loss: 0.5048 | Train Acc: 0.7448 | Val Acc: 0.7425 | Train ROC: 0.9441 | Val ROC: 0.8901 | Train AP: 0.9408 | Val AP: 0.8905\n",
      "Epoch [088/200] | Train Loss: 0.4839 | Val Loss: 0.4832 | Train Acc: 0.7469 | Val Acc: 0.7150 | Train ROC: 0.9449 | Val ROC: 0.8877 | Train AP: 0.9409 | Val AP: 0.8920\n",
      "Epoch [089/200] | Train Loss: 0.4819 | Val Loss: 0.5038 | Train Acc: 0.7456 | Val Acc: 0.7250 | Train ROC: 0.9452 | Val ROC: 0.8752 | Train AP: 0.9408 | Val AP: 0.8721\n",
      "Epoch [090/200] | Train Loss: 0.4751 | Val Loss: 0.5176 | Train Acc: 0.7488 | Val Acc: 0.7425 | Train ROC: 0.9528 | Val ROC: 0.9006 | Train AP: 0.9497 | Val AP: 0.8953\n",
      "Epoch [091/200] | Train Loss: 0.4750 | Val Loss: 0.5179 | Train Acc: 0.7505 | Val Acc: 0.7375 | Train ROC: 0.9483 | Val ROC: 0.8946 | Train AP: 0.9456 | Val AP: 0.9033\n",
      "Epoch [092/200] | Train Loss: 0.4656 | Val Loss: 0.5545 | Train Acc: 0.7549 | Val Acc: 0.7325 | Train ROC: 0.9531 | Val ROC: 0.8775 | Train AP: 0.9475 | Val AP: 0.8706\n",
      "Epoch [093/200] | Train Loss: 0.4715 | Val Loss: 0.5256 | Train Acc: 0.7496 | Val Acc: 0.7575 | Train ROC: 0.9538 | Val ROC: 0.9037 | Train AP: 0.9493 | Val AP: 0.9049\n",
      "Epoch [094/200] | Train Loss: 0.4757 | Val Loss: 0.5099 | Train Acc: 0.7480 | Val Acc: 0.7650 | Train ROC: 0.9516 | Val ROC: 0.9023 | Train AP: 0.9464 | Val AP: 0.8864\n",
      "Epoch [095/200] | Train Loss: 0.4684 | Val Loss: 0.5165 | Train Acc: 0.7445 | Val Acc: 0.7875 | Train ROC: 0.9518 | Val ROC: 0.9052 | Train AP: 0.9470 | Val AP: 0.9053\n",
      "Epoch [096/200] | Train Loss: 0.4735 | Val Loss: 0.5060 | Train Acc: 0.7549 | Val Acc: 0.7125 | Train ROC: 0.9555 | Val ROC: 0.8768 | Train AP: 0.9509 | Val AP: 0.8718\n",
      "Epoch [097/200] | Train Loss: 0.4678 | Val Loss: 0.5208 | Train Acc: 0.7569 | Val Acc: 0.7700 | Train ROC: 0.9574 | Val ROC: 0.9325 | Train AP: 0.9520 | Val AP: 0.9262\n",
      "Epoch [098/200] | Train Loss: 0.4635 | Val Loss: 0.4712 | Train Acc: 0.7560 | Val Acc: 0.7375 | Train ROC: 0.9573 | Val ROC: 0.9160 | Train AP: 0.9524 | Val AP: 0.9178\n",
      "Epoch [099/200] | Train Loss: 0.4722 | Val Loss: 0.4952 | Train Acc: 0.7526 | Val Acc: 0.7500 | Train ROC: 0.9573 | Val ROC: 0.9018 | Train AP: 0.9539 | Val AP: 0.9030\n",
      "Epoch [100/200] | Train Loss: 0.4664 | Val Loss: 0.4624 | Train Acc: 0.7530 | Val Acc: 0.7200 | Train ROC: 0.9557 | Val ROC: 0.9014 | Train AP: 0.9504 | Val AP: 0.9050\n",
      "Epoch [101/200] | Train Loss: 0.4595 | Val Loss: 0.5051 | Train Acc: 0.7545 | Val Acc: 0.7575 | Train ROC: 0.9585 | Val ROC: 0.9078 | Train AP: 0.9546 | Val AP: 0.9059\n",
      "Epoch [102/200] | Train Loss: 0.4641 | Val Loss: 0.5337 | Train Acc: 0.7548 | Val Acc: 0.7425 | Train ROC: 0.9613 | Val ROC: 0.9134 | Train AP: 0.9578 | Val AP: 0.9200\n",
      "Epoch [103/200] | Train Loss: 0.4688 | Val Loss: 0.5367 | Train Acc: 0.7519 | Val Acc: 0.7400 | Train ROC: 0.9602 | Val ROC: 0.8806 | Train AP: 0.9561 | Val AP: 0.8627\n",
      "Epoch [104/200] | Train Loss: 0.4664 | Val Loss: 0.4970 | Train Acc: 0.7565 | Val Acc: 0.7325 | Train ROC: 0.9643 | Val ROC: 0.9085 | Train AP: 0.9610 | Val AP: 0.9046\n",
      "Epoch [105/200] | Train Loss: 0.4614 | Val Loss: 0.5159 | Train Acc: 0.7555 | Val Acc: 0.7500 | Train ROC: 0.9616 | Val ROC: 0.9043 | Train AP: 0.9578 | Val AP: 0.9038\n",
      "Epoch [106/200] | Train Loss: 0.4678 | Val Loss: 0.5309 | Train Acc: 0.7481 | Val Acc: 0.7550 | Train ROC: 0.9629 | Val ROC: 0.9133 | Train AP: 0.9600 | Val AP: 0.9115\n",
      "Epoch [107/200] | Train Loss: 0.4551 | Val Loss: 0.4893 | Train Acc: 0.7605 | Val Acc: 0.7550 | Train ROC: 0.9637 | Val ROC: 0.9165 | Train AP: 0.9584 | Val AP: 0.9244\n",
      "Epoch [108/200] | Train Loss: 0.4671 | Val Loss: 0.4927 | Train Acc: 0.7606 | Val Acc: 0.7275 | Train ROC: 0.9654 | Val ROC: 0.8991 | Train AP: 0.9616 | Val AP: 0.8935\n",
      "Epoch [109/200] | Train Loss: 0.4613 | Val Loss: 0.4839 | Train Acc: 0.7550 | Val Acc: 0.7325 | Train ROC: 0.9663 | Val ROC: 0.9050 | Train AP: 0.9630 | Val AP: 0.8986\n",
      "Epoch [110/200] | Train Loss: 0.4713 | Val Loss: 0.5281 | Train Acc: 0.7494 | Val Acc: 0.7400 | Train ROC: 0.9634 | Val ROC: 0.8977 | Train AP: 0.9583 | Val AP: 0.8967\n",
      "Epoch [111/200] | Train Loss: 0.4610 | Val Loss: 0.5036 | Train Acc: 0.7536 | Val Acc: 0.7425 | Train ROC: 0.9644 | Val ROC: 0.9194 | Train AP: 0.9598 | Val AP: 0.9099\n",
      "Epoch [112/200] | Train Loss: 0.4679 | Val Loss: 0.5052 | Train Acc: 0.7416 | Val Acc: 0.7300 | Train ROC: 0.9647 | Val ROC: 0.9098 | Train AP: 0.9629 | Val AP: 0.9161\n",
      "Epoch [113/200] | Train Loss: 0.4651 | Val Loss: 0.5174 | Train Acc: 0.7601 | Val Acc: 0.7200 | Train ROC: 0.9688 | Val ROC: 0.8940 | Train AP: 0.9662 | Val AP: 0.8938\n",
      "Epoch [114/200] | Train Loss: 0.4565 | Val Loss: 0.4713 | Train Acc: 0.7556 | Val Acc: 0.7225 | Train ROC: 0.9651 | Val ROC: 0.9065 | Train AP: 0.9600 | Val AP: 0.9151\n",
      "Epoch [115/200] | Train Loss: 0.4680 | Val Loss: 0.4731 | Train Acc: 0.7524 | Val Acc: 0.7325 | Train ROC: 0.9692 | Val ROC: 0.9237 | Train AP: 0.9645 | Val AP: 0.9286\n",
      "Epoch [116/200] | Train Loss: 0.4584 | Val Loss: 0.5325 | Train Acc: 0.7561 | Val Acc: 0.7250 | Train ROC: 0.9714 | Val ROC: 0.8895 | Train AP: 0.9685 | Val AP: 0.8877\n",
      "Epoch [117/200] | Train Loss: 0.4608 | Val Loss: 0.4944 | Train Acc: 0.7522 | Val Acc: 0.7225 | Train ROC: 0.9701 | Val ROC: 0.8852 | Train AP: 0.9660 | Val AP: 0.8719\n",
      "Epoch [118/200] | Train Loss: 0.4549 | Val Loss: 0.5120 | Train Acc: 0.7576 | Val Acc: 0.7475 | Train ROC: 0.9693 | Val ROC: 0.8936 | Train AP: 0.9648 | Val AP: 0.8790\n",
      "Epoch [119/200] | Train Loss: 0.4595 | Val Loss: 0.5472 | Train Acc: 0.7579 | Val Acc: 0.6800 | Train ROC: 0.9699 | Val ROC: 0.9166 | Train AP: 0.9650 | Val AP: 0.9197\n",
      "Epoch [120/200] | Train Loss: 0.4565 | Val Loss: 0.4909 | Train Acc: 0.7516 | Val Acc: 0.7175 | Train ROC: 0.9683 | Val ROC: 0.8969 | Train AP: 0.9624 | Val AP: 0.8926\n",
      "Epoch [121/200] | Train Loss: 0.4543 | Val Loss: 0.5042 | Train Acc: 0.7588 | Val Acc: 0.7600 | Train ROC: 0.9721 | Val ROC: 0.9228 | Train AP: 0.9685 | Val AP: 0.9258\n",
      "Epoch [122/200] | Train Loss: 0.4591 | Val Loss: 0.5043 | Train Acc: 0.7589 | Val Acc: 0.7475 | Train ROC: 0.9743 | Val ROC: 0.9188 | Train AP: 0.9706 | Val AP: 0.9101\n",
      "Epoch [123/200] | Train Loss: 0.4566 | Val Loss: 0.4825 | Train Acc: 0.7521 | Val Acc: 0.7275 | Train ROC: 0.9730 | Val ROC: 0.9058 | Train AP: 0.9705 | Val AP: 0.9119\n",
      "Epoch [124/200] | Train Loss: 0.4514 | Val Loss: 0.4694 | Train Acc: 0.7516 | Val Acc: 0.7250 | Train ROC: 0.9723 | Val ROC: 0.9213 | Train AP: 0.9679 | Val AP: 0.9283\n",
      "Epoch [125/200] | Train Loss: 0.4462 | Val Loss: 0.4883 | Train Acc: 0.7531 | Val Acc: 0.7275 | Train ROC: 0.9728 | Val ROC: 0.8922 | Train AP: 0.9686 | Val AP: 0.8760\n",
      "Epoch [126/200] | Train Loss: 0.4593 | Val Loss: 0.4943 | Train Acc: 0.7588 | Val Acc: 0.7225 | Train ROC: 0.9727 | Val ROC: 0.8861 | Train AP: 0.9689 | Val AP: 0.8740\n",
      "Epoch [127/200] | Train Loss: 0.4543 | Val Loss: 0.4926 | Train Acc: 0.7551 | Val Acc: 0.7600 | Train ROC: 0.9745 | Val ROC: 0.9039 | Train AP: 0.9699 | Val AP: 0.9060\n",
      "Epoch [128/200] | Train Loss: 0.4540 | Val Loss: 0.5151 | Train Acc: 0.7484 | Val Acc: 0.7625 | Train ROC: 0.9723 | Val ROC: 0.9336 | Train AP: 0.9692 | Val AP: 0.9352\n",
      "Epoch [129/200] | Train Loss: 0.4581 | Val Loss: 0.5339 | Train Acc: 0.7562 | Val Acc: 0.7150 | Train ROC: 0.9748 | Val ROC: 0.8866 | Train AP: 0.9716 | Val AP: 0.8854\n",
      "Epoch [130/200] | Train Loss: 0.4544 | Val Loss: 0.4871 | Train Acc: 0.7581 | Val Acc: 0.7600 | Train ROC: 0.9775 | Val ROC: 0.9222 | Train AP: 0.9738 | Val AP: 0.9194\n",
      "Epoch [131/200] | Train Loss: 0.4485 | Val Loss: 0.4714 | Train Acc: 0.7519 | Val Acc: 0.7300 | Train ROC: 0.9738 | Val ROC: 0.9181 | Train AP: 0.9695 | Val AP: 0.9192\n",
      "Epoch [132/200] | Train Loss: 0.4514 | Val Loss: 0.5006 | Train Acc: 0.7572 | Val Acc: 0.7200 | Train ROC: 0.9767 | Val ROC: 0.9143 | Train AP: 0.9710 | Val AP: 0.9216\n",
      "Epoch [133/200] | Train Loss: 0.4564 | Val Loss: 0.5161 | Train Acc: 0.7585 | Val Acc: 0.7625 | Train ROC: 0.9756 | Val ROC: 0.9310 | Train AP: 0.9705 | Val AP: 0.9308\n",
      "Epoch [134/200] | Train Loss: 0.4479 | Val Loss: 0.4855 | Train Acc: 0.7566 | Val Acc: 0.7550 | Train ROC: 0.9743 | Val ROC: 0.9275 | Train AP: 0.9673 | Val AP: 0.9299\n",
      "Epoch [135/200] | Train Loss: 0.4468 | Val Loss: 0.5277 | Train Acc: 0.7642 | Val Acc: 0.7750 | Train ROC: 0.9765 | Val ROC: 0.9320 | Train AP: 0.9737 | Val AP: 0.9285\n",
      "Epoch [136/200] | Train Loss: 0.4465 | Val Loss: 0.5080 | Train Acc: 0.7555 | Val Acc: 0.7425 | Train ROC: 0.9764 | Val ROC: 0.9183 | Train AP: 0.9716 | Val AP: 0.9244\n",
      "Epoch [137/200] | Train Loss: 0.4523 | Val Loss: 0.5660 | Train Acc: 0.7548 | Val Acc: 0.7225 | Train ROC: 0.9794 | Val ROC: 0.9178 | Train AP: 0.9763 | Val AP: 0.9220\n",
      "Epoch [138/200] | Train Loss: 0.4498 | Val Loss: 0.5111 | Train Acc: 0.7491 | Val Acc: 0.7275 | Train ROC: 0.9785 | Val ROC: 0.8896 | Train AP: 0.9739 | Val AP: 0.8940\n",
      "Epoch [139/200] | Train Loss: 0.4418 | Val Loss: 0.4848 | Train Acc: 0.7581 | Val Acc: 0.7450 | Train ROC: 0.9775 | Val ROC: 0.9254 | Train AP: 0.9730 | Val AP: 0.9263\n",
      "Epoch [140/200] | Train Loss: 0.4513 | Val Loss: 0.4836 | Train Acc: 0.7482 | Val Acc: 0.7225 | Train ROC: 0.9752 | Val ROC: 0.9052 | Train AP: 0.9693 | Val AP: 0.9092\n",
      "Epoch [141/200] | Train Loss: 0.4508 | Val Loss: 0.4889 | Train Acc: 0.7539 | Val Acc: 0.7725 | Train ROC: 0.9785 | Val ROC: 0.9391 | Train AP: 0.9746 | Val AP: 0.9358\n",
      "Epoch [142/200] | Train Loss: 0.4359 | Val Loss: 0.5011 | Train Acc: 0.7555 | Val Acc: 0.7550 | Train ROC: 0.9784 | Val ROC: 0.9294 | Train AP: 0.9746 | Val AP: 0.9365\n",
      "Epoch [143/200] | Train Loss: 0.4491 | Val Loss: 0.4678 | Train Acc: 0.7609 | Val Acc: 0.7125 | Train ROC: 0.9792 | Val ROC: 0.9223 | Train AP: 0.9763 | Val AP: 0.9279\n",
      "Epoch [144/200] | Train Loss: 0.4464 | Val Loss: 0.4716 | Train Acc: 0.7574 | Val Acc: 0.7525 | Train ROC: 0.9790 | Val ROC: 0.9279 | Train AP: 0.9749 | Val AP: 0.9145\n",
      "Epoch [145/200] | Train Loss: 0.4461 | Val Loss: 0.5004 | Train Acc: 0.7599 | Val Acc: 0.7550 | Train ROC: 0.9816 | Val ROC: 0.9250 | Train AP: 0.9774 | Val AP: 0.9202\n",
      "Epoch [146/200] | Train Loss: 0.4415 | Val Loss: 0.4998 | Train Acc: 0.7541 | Val Acc: 0.7225 | Train ROC: 0.9792 | Val ROC: 0.9045 | Train AP: 0.9757 | Val AP: 0.9140\n",
      "Epoch [147/200] | Train Loss: 0.4427 | Val Loss: 0.5006 | Train Acc: 0.7546 | Val Acc: 0.7650 | Train ROC: 0.9809 | Val ROC: 0.9224 | Train AP: 0.9770 | Val AP: 0.9227\n",
      "Epoch [148/200] | Train Loss: 0.4449 | Val Loss: 0.4848 | Train Acc: 0.7538 | Val Acc: 0.7400 | Train ROC: 0.9801 | Val ROC: 0.9229 | Train AP: 0.9760 | Val AP: 0.9243\n",
      "Epoch [149/200] | Train Loss: 0.4504 | Val Loss: 0.5192 | Train Acc: 0.7614 | Val Acc: 0.7225 | Train ROC: 0.9781 | Val ROC: 0.9117 | Train AP: 0.9725 | Val AP: 0.9043\n",
      "Epoch [150/200] | Train Loss: 0.4455 | Val Loss: 0.4833 | Train Acc: 0.7500 | Val Acc: 0.7250 | Train ROC: 0.9801 | Val ROC: 0.8926 | Train AP: 0.9772 | Val AP: 0.8991\n",
      "Epoch [151/200] | Train Loss: 0.4434 | Val Loss: 0.5142 | Train Acc: 0.7541 | Val Acc: 0.7375 | Train ROC: 0.9821 | Val ROC: 0.9158 | Train AP: 0.9784 | Val AP: 0.9193\n",
      "Epoch [152/200] | Train Loss: 0.4442 | Val Loss: 0.4749 | Train Acc: 0.7548 | Val Acc: 0.7300 | Train ROC: 0.9819 | Val ROC: 0.9170 | Train AP: 0.9765 | Val AP: 0.9153\n",
      "Epoch [153/200] | Train Loss: 0.4413 | Val Loss: 0.5029 | Train Acc: 0.7478 | Val Acc: 0.7575 | Train ROC: 0.9809 | Val ROC: 0.9338 | Train AP: 0.9765 | Val AP: 0.9381\n",
      "Epoch [154/200] | Train Loss: 0.4415 | Val Loss: 0.5047 | Train Acc: 0.7550 | Val Acc: 0.7375 | Train ROC: 0.9822 | Val ROC: 0.9327 | Train AP: 0.9783 | Val AP: 0.9403\n",
      "Epoch [155/200] | Train Loss: 0.4456 | Val Loss: 0.4861 | Train Acc: 0.7554 | Val Acc: 0.7075 | Train ROC: 0.9830 | Val ROC: 0.9103 | Train AP: 0.9798 | Val AP: 0.9081\n",
      "Epoch [156/200] | Train Loss: 0.4510 | Val Loss: 0.4925 | Train Acc: 0.7555 | Val Acc: 0.7725 | Train ROC: 0.9845 | Val ROC: 0.9263 | Train AP: 0.9807 | Val AP: 0.9179\n",
      "Epoch [157/200] | Train Loss: 0.4406 | Val Loss: 0.4881 | Train Acc: 0.7568 | Val Acc: 0.7225 | Train ROC: 0.9825 | Val ROC: 0.9059 | Train AP: 0.9797 | Val AP: 0.9067\n",
      "Epoch [158/200] | Train Loss: 0.4439 | Val Loss: 0.4853 | Train Acc: 0.7584 | Val Acc: 0.7275 | Train ROC: 0.9860 | Val ROC: 0.9247 | Train AP: 0.9840 | Val AP: 0.9345\n",
      "Epoch [159/200] | Train Loss: 0.4417 | Val Loss: 0.5248 | Train Acc: 0.7515 | Val Acc: 0.7400 | Train ROC: 0.9851 | Val ROC: 0.9274 | Train AP: 0.9822 | Val AP: 0.9336\n",
      "Epoch [160/200] | Train Loss: 0.4411 | Val Loss: 0.4947 | Train Acc: 0.7552 | Val Acc: 0.7675 | Train ROC: 0.9857 | Val ROC: 0.9309 | Train AP: 0.9826 | Val AP: 0.9380\n",
      "Epoch [161/200] | Train Loss: 0.4427 | Val Loss: 0.4561 | Train Acc: 0.7480 | Val Acc: 0.7425 | Train ROC: 0.9846 | Val ROC: 0.9169 | Train AP: 0.9809 | Val AP: 0.9153\n",
      "Epoch [162/200] | Train Loss: 0.4335 | Val Loss: 0.5310 | Train Acc: 0.7541 | Val Acc: 0.7275 | Train ROC: 0.9834 | Val ROC: 0.9113 | Train AP: 0.9777 | Val AP: 0.9147\n",
      "Epoch [163/200] | Train Loss: 0.4339 | Val Loss: 0.4691 | Train Acc: 0.7494 | Val Acc: 0.7175 | Train ROC: 0.9848 | Val ROC: 0.9047 | Train AP: 0.9819 | Val AP: 0.9028\n",
      "Epoch [164/200] | Train Loss: 0.4463 | Val Loss: 0.4806 | Train Acc: 0.7494 | Val Acc: 0.7300 | Train ROC: 0.9830 | Val ROC: 0.9002 | Train AP: 0.9797 | Val AP: 0.8836\n",
      "Epoch [165/200] | Train Loss: 0.4415 | Val Loss: 0.5118 | Train Acc: 0.7566 | Val Acc: 0.7450 | Train ROC: 0.9843 | Val ROC: 0.9322 | Train AP: 0.9804 | Val AP: 0.9406\n",
      "Epoch [166/200] | Train Loss: 0.4361 | Val Loss: 0.4874 | Train Acc: 0.7556 | Val Acc: 0.7425 | Train ROC: 0.9868 | Val ROC: 0.9116 | Train AP: 0.9852 | Val AP: 0.9062\n",
      "Epoch [167/200] | Train Loss: 0.4322 | Val Loss: 0.4787 | Train Acc: 0.7556 | Val Acc: 0.7250 | Train ROC: 0.9842 | Val ROC: 0.9247 | Train AP: 0.9806 | Val AP: 0.9317\n",
      "Epoch [168/200] | Train Loss: 0.4343 | Val Loss: 0.4684 | Train Acc: 0.7514 | Val Acc: 0.7625 | Train ROC: 0.9866 | Val ROC: 0.9262 | Train AP: 0.9839 | Val AP: 0.9272\n",
      "Epoch [169/200] | Train Loss: 0.4400 | Val Loss: 0.4637 | Train Acc: 0.7566 | Val Acc: 0.7300 | Train ROC: 0.9882 | Val ROC: 0.9195 | Train AP: 0.9858 | Val AP: 0.9233\n",
      "Epoch [170/200] | Train Loss: 0.4514 | Val Loss: 0.5136 | Train Acc: 0.7516 | Val Acc: 0.7450 | Train ROC: 0.9866 | Val ROC: 0.9304 | Train AP: 0.9829 | Val AP: 0.9366\n",
      "Epoch [171/200] | Train Loss: 0.4343 | Val Loss: 0.5362 | Train Acc: 0.7524 | Val Acc: 0.7250 | Train ROC: 0.9868 | Val ROC: 0.9178 | Train AP: 0.9845 | Val AP: 0.9249\n",
      "Epoch [172/200] | Train Loss: 0.4330 | Val Loss: 0.5098 | Train Acc: 0.7514 | Val Acc: 0.7125 | Train ROC: 0.9878 | Val ROC: 0.9179 | Train AP: 0.9856 | Val AP: 0.9222\n",
      "Epoch [173/200] | Train Loss: 0.4381 | Val Loss: 0.4743 | Train Acc: 0.7578 | Val Acc: 0.7400 | Train ROC: 0.9870 | Val ROC: 0.9314 | Train AP: 0.9844 | Val AP: 0.9395\n",
      "Epoch [174/200] | Train Loss: 0.4341 | Val Loss: 0.4957 | Train Acc: 0.7608 | Val Acc: 0.7200 | Train ROC: 0.9862 | Val ROC: 0.9234 | Train AP: 0.9827 | Val AP: 0.9341\n",
      "Epoch [175/200] | Train Loss: 0.4375 | Val Loss: 0.4983 | Train Acc: 0.7616 | Val Acc: 0.7250 | Train ROC: 0.9867 | Val ROC: 0.9147 | Train AP: 0.9830 | Val AP: 0.9265\n",
      "Epoch [176/200] | Train Loss: 0.4302 | Val Loss: 0.4649 | Train Acc: 0.7591 | Val Acc: 0.7075 | Train ROC: 0.9881 | Val ROC: 0.9180 | Train AP: 0.9856 | Val AP: 0.9255\n",
      "Epoch [177/200] | Train Loss: 0.4371 | Val Loss: 0.4953 | Train Acc: 0.7612 | Val Acc: 0.7600 | Train ROC: 0.9895 | Val ROC: 0.9259 | Train AP: 0.9876 | Val AP: 0.9126\n",
      "Epoch [178/200] | Train Loss: 0.4407 | Val Loss: 0.5077 | Train Acc: 0.7574 | Val Acc: 0.7625 | Train ROC: 0.9888 | Val ROC: 0.9192 | Train AP: 0.9873 | Val AP: 0.9239\n",
      "Epoch [179/200] | Train Loss: 0.4377 | Val Loss: 0.4923 | Train Acc: 0.7574 | Val Acc: 0.7375 | Train ROC: 0.9890 | Val ROC: 0.9331 | Train AP: 0.9857 | Val AP: 0.9400\n",
      "Epoch [180/200] | Train Loss: 0.4375 | Val Loss: 0.5417 | Train Acc: 0.7572 | Val Acc: 0.7075 | Train ROC: 0.9864 | Val ROC: 0.9202 | Train AP: 0.9837 | Val AP: 0.9317\n",
      "Epoch [181/200] | Train Loss: 0.4344 | Val Loss: 0.5317 | Train Acc: 0.7595 | Val Acc: 0.7550 | Train ROC: 0.9895 | Val ROC: 0.9367 | Train AP: 0.9876 | Val AP: 0.9430\n",
      "Epoch [182/200] | Train Loss: 0.4320 | Val Loss: 0.5025 | Train Acc: 0.7564 | Val Acc: 0.7275 | Train ROC: 0.9869 | Val ROC: 0.9248 | Train AP: 0.9830 | Val AP: 0.9284\n",
      "Epoch [183/200] | Train Loss: 0.4361 | Val Loss: 0.4738 | Train Acc: 0.7545 | Val Acc: 0.7300 | Train ROC: 0.9892 | Val ROC: 0.9133 | Train AP: 0.9866 | Val AP: 0.9149\n",
      "Epoch [184/200] | Train Loss: 0.4279 | Val Loss: 0.4816 | Train Acc: 0.7556 | Val Acc: 0.7125 | Train ROC: 0.9880 | Val ROC: 0.9030 | Train AP: 0.9847 | Val AP: 0.9071\n",
      "Epoch [185/200] | Train Loss: 0.4267 | Val Loss: 0.4751 | Train Acc: 0.7572 | Val Acc: 0.7625 | Train ROC: 0.9895 | Val ROC: 0.9400 | Train AP: 0.9870 | Val AP: 0.9510\n",
      "Epoch [186/200] | Train Loss: 0.4310 | Val Loss: 0.4730 | Train Acc: 0.7596 | Val Acc: 0.7500 | Train ROC: 0.9882 | Val ROC: 0.9230 | Train AP: 0.9853 | Val AP: 0.9265\n",
      "Epoch [187/200] | Train Loss: 0.4388 | Val Loss: 0.5071 | Train Acc: 0.7544 | Val Acc: 0.7725 | Train ROC: 0.9882 | Val ROC: 0.9305 | Train AP: 0.9850 | Val AP: 0.9209\n",
      "Epoch [188/200] | Train Loss: 0.4263 | Val Loss: 0.4868 | Train Acc: 0.7551 | Val Acc: 0.7175 | Train ROC: 0.9897 | Val ROC: 0.9149 | Train AP: 0.9858 | Val AP: 0.9232\n",
      "Epoch [189/200] | Train Loss: 0.4324 | Val Loss: 0.4786 | Train Acc: 0.7556 | Val Acc: 0.7350 | Train ROC: 0.9868 | Val ROC: 0.9313 | Train AP: 0.9817 | Val AP: 0.9347\n",
      "Epoch [190/200] | Train Loss: 0.4284 | Val Loss: 0.4687 | Train Acc: 0.7554 | Val Acc: 0.7125 | Train ROC: 0.9889 | Val ROC: 0.9312 | Train AP: 0.9863 | Val AP: 0.9399\n",
      "Epoch [191/200] | Train Loss: 0.4306 | Val Loss: 0.4558 | Train Acc: 0.7634 | Val Acc: 0.7475 | Train ROC: 0.9900 | Val ROC: 0.9418 | Train AP: 0.9879 | Val AP: 0.9478\n",
      "Epoch [192/200] | Train Loss: 0.4208 | Val Loss: 0.4729 | Train Acc: 0.7659 | Val Acc: 0.7600 | Train ROC: 0.9896 | Val ROC: 0.9197 | Train AP: 0.9863 | Val AP: 0.9188\n",
      "Epoch [193/200] | Train Loss: 0.4309 | Val Loss: 0.4807 | Train Acc: 0.7529 | Val Acc: 0.7425 | Train ROC: 0.9894 | Val ROC: 0.9233 | Train AP: 0.9866 | Val AP: 0.9330\n",
      "Epoch [194/200] | Train Loss: 0.4395 | Val Loss: 0.4968 | Train Acc: 0.7580 | Val Acc: 0.7500 | Train ROC: 0.9903 | Val ROC: 0.9334 | Train AP: 0.9881 | Val AP: 0.9395\n",
      "Epoch [195/200] | Train Loss: 0.4248 | Val Loss: 0.4664 | Train Acc: 0.7530 | Val Acc: 0.7425 | Train ROC: 0.9901 | Val ROC: 0.9175 | Train AP: 0.9878 | Val AP: 0.9232\n",
      "Epoch [196/200] | Train Loss: 0.4302 | Val Loss: 0.4663 | Train Acc: 0.7549 | Val Acc: 0.7275 | Train ROC: 0.9896 | Val ROC: 0.9065 | Train AP: 0.9866 | Val AP: 0.9126\n",
      "Epoch [197/200] | Train Loss: 0.4289 | Val Loss: 0.4911 | Train Acc: 0.7570 | Val Acc: 0.7775 | Train ROC: 0.9908 | Val ROC: 0.9392 | Train AP: 0.9891 | Val AP: 0.9462\n",
      "Epoch [198/200] | Train Loss: 0.4293 | Val Loss: 0.4724 | Train Acc: 0.7576 | Val Acc: 0.7500 | Train ROC: 0.9918 | Val ROC: 0.9291 | Train AP: 0.9903 | Val AP: 0.9293\n",
      "Epoch [199/200] | Train Loss: 0.4332 | Val Loss: 0.4657 | Train Acc: 0.7484 | Val Acc: 0.7250 | Train ROC: 0.9901 | Val ROC: 0.9165 | Train AP: 0.9881 | Val AP: 0.9238\n",
      "Epoch [200/200] | Train Loss: 0.4288 | Val Loss: 0.4647 | Train Acc: 0.7566 | Val Acc: 0.7325 | Train ROC: 0.9891 | Val ROC: 0.9223 | Train AP: 0.9853 | Val AP: 0.9237\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass on training adjacency\n",
    "    adj_recon = model(x, pos_enc, train_adj)\n",
    "\n",
    "    # Compute sampled loss (balanced positives/negatives)\n",
    "    total_loss, recon_loss, kl_loss = sampled_vae_loss(\n",
    "        adj_recon, train_adj, model.z_mean, model.z_logvar, num_samples=8000\n",
    "    )\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Metrics (sampled)\n",
    "    roc_train, ap_train, acc_train = sampled_metrics(adj_recon, train_adj, num_samples=8000)\n",
    "\n",
    "    # ---------- VALIDATION ----------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        adj_val_recon = model(x, pos_enc, val_adj)\n",
    "        val_loss, _, _ = sampled_vae_loss(\n",
    "            adj_val_recon, val_adj, model.z_mean, model.z_logvar, num_samples=400\n",
    "        )\n",
    "        roc_val, ap_val, acc_val = sampled_metrics(adj_val_recon, val_adj, num_samples=400)\n",
    "\n",
    "    # ---------- LOGGING ----------\n",
    "    print(\n",
    "        f\"Epoch [{epoch:03d}/{num_epochs}] | \"\n",
    "        f\"Train Loss: {total_loss.item():.4f} | Val Loss: {val_loss.item():.4f} | \"\n",
    "        f\"Train Acc: {acc_train:.4f} | Val Acc: {acc_val:.4f} | \"\n",
    "        f\"Train ROC: {roc_train:.4f} | Val ROC: {roc_val:.4f} | \"\n",
    "        f\"Train AP: {ap_train:.4f} | Val AP: {ap_val:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9630f7b8-bcc7-4942-8e45-c9863a4edd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "def test_model(model, x, pos_e, test_adj, num_samples):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        adj_test_recon = model(x, pos_enc, test_adj)\n",
    "    \n",
    "        # Balanced sampled VAE loss\n",
    "        test_loss, _, _ = sampled_vae_loss(\n",
    "            adj_test_recon, test_adj, model.z_mean, model.z_logvar, num_samples=1054\n",
    "\n",
    "        )\n",
    "    \n",
    "        # Sampled metrics (balanced)\n",
    "        roc_test, ap_test, acc_test = sampled_metrics(\n",
    "            adj_test_recon, test_adj, num_samples=num_samples\n",
    "        )\n",
    "    return test_loss, acc_test, roc_test, ap_test\n",
    "\n",
    "exps = 20\n",
    "losses = []\n",
    "accs = []\n",
    "rocs = []\n",
    "precs = []\n",
    "for i in range(exps):\n",
    "    torch.manual_seed(42 + i)\n",
    "    t_loss, t_acc, t_roc, t_prec = test_model(model, x, pos_enc, test_adj, num_samples=1000)\n",
    "    losses.append(t_loss.detach().cpu().item())\n",
    "    accs.append(t_acc)\n",
    "    rocs.append(t_roc)\n",
    "    precs.append(t_prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd3453b-4cb2-4640-91a4-6356b0a2c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "accs = np.array(accs)\n",
    "rocs = np.array(rocs)\n",
    "precs = np.array(precs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf805990-23a5-473a-bf54-7bdeb6b91e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL TEST RESULTS ===\n",
      "Test Loss: 0.4981 +- 0.011562777382434627 | \n",
      "Test Acc: 0.7343 +- 0.01081711606667878 | \n",
      "ROC-AUC: 0.9161 +- 0.005897844162064628 |\n",
      "AP: 0.9235 +- 0.00753070491905668\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== FINAL TEST RESULTS ===\")\n",
    "print(\n",
    "    f\"Test Loss: {losses.mean():.4f} +- {losses.std()} | \\n\"\n",
    "    f\"Test Acc: {accs.mean():.4f} +- {accs.std()} | \\n\"\n",
    "    f\"ROC-AUC: {rocs.mean():.4f} +- {rocs.std()} |\\nAP: {precs.mean():.4f} +- {precs.std()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb76eaf-039e-4a37-ac09-4c0c4bcce5d1",
   "metadata": {},
   "source": [
    "=== FINAL TEST RESULTS ===\n",
    "kl: 0.005\n",
    "n_heads = 4\n",
    "dim = 128\n",
    "n_layers = 4\n",
    "\n",
    "Test Loss: 0.4868 +- 0.009812361955942385 | \n",
    "Test Acc: 0.7414 +- 0.008182145195485114 | \n",
    "ROC-AUC: 0.9176 +- 0.005479434469906512 |\n",
    "AP: 0.9240 +- 0.007316556545360688"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaced33-c391-4aaf-b455-1bd0f5a6181a",
   "metadata": {},
   "source": [
    "kl: 0.0005\n",
    "lr: 0.0001\n",
    "\n",
    "=== FINAL TEST RESULTS ===\n",
    "Test Loss: 0.5380 +- 0.014766905705631252 | \n",
    "Test Acc: 0.7196 +- 0.008816320094007483 | \n",
    "ROC-AUC: 0.8679 +- 0.007809779471278348 |\n",
    "AP: 0.8753 +- 0.009753158063202054"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ca9e4-611e-4d75-8083-cc9aa089c0ba",
   "metadata": {},
   "source": [
    "# Best Result\n",
    "\n",
    "kl: 0.0005\n",
    "lr: 1e-3\n",
    "n_heads = 4, n_layers=4, pos_dim=128, hidden_dim=128\n",
    "\n",
    "=== FINAL TEST RESULTS ===\n",
    "Test Loss: 0.4818 +- 0.007868968959063699 | \n",
    "Test Acc: 0.7419 +- 0.01077253452071518 | \n",
    "ROC-AUC: 0.9204 +- 0.006907713010830738 |\n",
    "AP: 0.9266 +- 0.008169060892064308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2299fb93-c513-4298-8c92-171d1378c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"92_4H_4L_128.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51a1a22-668e-413c-acce-a5f8cc073676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:335: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Q, _ = normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:335: RuntimeWarning: overflow encountered in matmul\n",
      "  Q, _ = normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:335: RuntimeWarning: invalid value encountered in matmul\n",
      "  Q, _ = normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Q, _ = normalizer(A.T @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  Q, _ = normalizer(A.T @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  Q, _ = normalizer(A.T @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:340: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Q, _ = qr_normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:340: RuntimeWarning: overflow encountered in matmul\n",
      "  Q, _ = qr_normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:340: RuntimeWarning: invalid value encountered in matmul\n",
      "  Q, _ = qr_normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:533: RuntimeWarning: divide by zero encountered in matmul\n",
      "  B = Q.T @ M\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:533: RuntimeWarning: overflow encountered in matmul\n",
      "  B = Q.T @ M\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:533: RuntimeWarning: invalid value encountered in matmul\n",
      "  B = Q.T @ M\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:547: RuntimeWarning: divide by zero encountered in matmul\n",
      "  U = Q @ Uhat\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:547: RuntimeWarning: overflow encountered in matmul\n",
      "  U = Q @ Uhat\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:547: RuntimeWarning: invalid value encountered in matmul\n",
      "  U = Q @ Uhat\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:335: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Q, _ = normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:335: RuntimeWarning: overflow encountered in matmul\n",
      "  Q, _ = normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:335: RuntimeWarning: invalid value encountered in matmul\n",
      "  Q, _ = normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Q, _ = normalizer(A.T @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  Q, _ = normalizer(A.T @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  Q, _ = normalizer(A.T @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:340: RuntimeWarning: divide by zero encountered in matmul\n",
      "  Q, _ = qr_normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:340: RuntimeWarning: overflow encountered in matmul\n",
      "  Q, _ = qr_normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:340: RuntimeWarning: invalid value encountered in matmul\n",
      "  Q, _ = qr_normalizer(A @ Q)\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:533: RuntimeWarning: divide by zero encountered in matmul\n",
      "  B = Q.T @ M\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:533: RuntimeWarning: overflow encountered in matmul\n",
      "  B = Q.T @ M\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:533: RuntimeWarning: invalid value encountered in matmul\n",
      "  B = Q.T @ M\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:547: RuntimeWarning: divide by zero encountered in matmul\n",
      "  U = Q @ Uhat\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:547: RuntimeWarning: overflow encountered in matmul\n",
      "  U = Q @ Uhat\n",
      "/Users/sid/COLLEGE_MATERIALS/Research/research/lib/python3.9/site-packages/sklearn/utils/extmath.py:547: RuntimeWarning: invalid value encountered in matmul\n",
      "  U = Q @ Uhat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'latent_2d': '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/latent_tsne_2d_20251021_093826.png',\n",
       " 'latent_3d': '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/latent_tsne_3d_20251021_093839.html',\n",
       " 'adj_comparison': '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/adjacency_comparison_20251021_093850.png',\n",
       " 'link_confidence': '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/link_confidence_20251021_093902.png',\n",
       " 'attention_stats': '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/attention_stats_20251021_093906.png',\n",
       " 'attention_layer_files': ['/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/attention_layer_0_20251021_093924.png',\n",
       "  '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/attention_layer_1_20251021_093956.png',\n",
       "  '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/attention_layer_2_20251021_094028.png',\n",
       "  '/Users/sid/COLLEGE_MATERIALS/Research/vgae_pytorch/visualizations/attention_layer_3_20251021_094059.png']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from analysis_model import run_full_analysis\n",
    "run_full_analysis(model, x, pos_enc, train_adj, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9c5037-58bb-4104-a935-f5b68bfe7dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
